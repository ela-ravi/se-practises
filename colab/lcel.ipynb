{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4722386",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "049ff22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_5(x):\n",
    "    return x+5\n",
    "def sub_5(x):\n",
    "    return x-5\n",
    "def mul_5(x):\n",
    "    return x*5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80c5d0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_5_runnable = RunnableLambda(add_5)\n",
    "sub_5_runnable = RunnableLambda(sub_5)\n",
    "mul_5_runnable = RunnableLambda(mul_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f72c51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = add_5_runnable.__or__(mul_5_runnable).__or__(sub_5_runnable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d370642d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = chain.invoke(5)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14d30fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain2 = add_5_runnable | sub_5_runnable | mul_5_runnable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca894309",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain2.invoke(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6900fbd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "84632670",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['topic'], input_types={}, partial_variables={}, template='Give me a small report on {topic}')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template = \"Give me a small report on {topic}\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"topic\"])\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "784fd7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_models import ChatOllama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "141c39c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2w/cqz3z_yn04n_r99f8b5w1frc0000gp/T/ipykernel_90647/3555080338.py:1: LangChainDeprecationWarning: The class `ChatOllama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the `langchain-ollama package and should be used instead. To use it run `pip install -U `langchain-ollama` and import as `from `langchain_ollama import ChatOllama``.\n",
      "  llm = ChatOllama(model=\"llama3.1:8b\")\n"
     ]
    }
   ],
   "source": [
    "llm = ChatOllama(model=\"llama3.1:8b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "227f54d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['topic'], input_types={}, partial_variables={}, template='Give me a small report on {topic}')\n",
       "| ChatOllama(model='llama3.1:8b')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = prompt | llm\n",
    "chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "29ad461d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"**Retrieval-Augmented Generation (RAG)**\\n\\n**Overview:**\\nRetrieval-Augmentation-Generation (RAG) is a recent paradigm in natural language processing (NLP) that combines the strengths of two popular techniques: retrieval-based models and generation-based models. This approach has shown impressive results in various downstream tasks, such as question answering, text classification, and machine translation.\\n\\n**Key Components:**\\n\\n1. **Retrieval Module:** This module retrieves relevant documents or passages from a large database based on the input query or prompt.\\n2. **Augmentation Module:** The retrieved documents are then augmented with additional information, such as context, metadata, or auxiliary tasks, to enhance the model's understanding of the input.\\n3. **Generation Module:** Finally, the augmented document is fed into a generation module, which produces the final output.\\n\\n**Benefits:**\\n\\n1. **Improved performance**: By leveraging external knowledge and augmenting it with contextual information, RAG models can achieve state-of-the-art results in various NLP tasks.\\n2. **Knowledge distillation**: RAG enables knowledge distillation from large datasets to smaller ones, making it more efficient for training and deployment.\\n3. **Flexibility**: The modular design of RAG allows for easy integration with other architectures and techniques.\\n\\n**Applications:**\\n\\n1. **Question Answering (QA)**: RAG has achieved state-of-the-art results in several QA benchmarks, outperforming traditional generation-based models.\\n2. **Text Classification**: RAG can improve text classification performance by incorporating relevant documents and augmenting them with contextual information.\\n3. **Machine Translation**: RAG can be used to enhance machine translation systems by retrieving relevant bilingual texts and augmenting them with context.\\n\\n**Challenges:**\\n\\n1. **Computational Cost**: Retrieval and augmentation processes can be computationally expensive, especially when dealing with large databases.\\n2. **Training Complexity**: Training RAG models requires careful tuning of hyperparameters and model architectures to ensure effective knowledge retrieval and augmentation.\\n3. **Data Quality**: The quality of the retrieved documents and augmented data significantly impacts the performance of RAG models.\\n\\n**Current Research:**\\n\\n1. **Efficient Retrieval Methods**: Researchers are exploring efficient retrieval methods, such as sparse indexing and caching, to reduce computational costs.\\n2. **Transfer Learning**: Transfer learning techniques are being applied to RAG models to leverage knowledge from pre-trained language models.\\n3. **Explainability and Evaluation**: Studies are focusing on developing evaluation metrics and explanation methods for RAG models to better understand their behavior.\\n\\nOverall, Retrieval-Augmentation-Generation (RAG) has the potential to revolutionize NLP by combining the strengths of retrieval-based and generation-based models.\", additional_kwargs={}, response_metadata={'model': 'llama3.1:8b', 'created_at': '2025-11-22T09:21:10.908714Z', 'message': {'role': 'assistant', 'content': ''}, 'done': True, 'done_reason': 'stop', 'total_duration': 23002127833, 'load_duration': 148418417, 'prompt_eval_count': 23, 'prompt_eval_duration': 207870500, 'eval_count': 551, 'eval_duration': 22349847637}, id='lc_run--dbfd33f0-a378-4db0-9726-b22eb6d2a72d-0')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"Retrieval Augmentatiion Generation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c979d3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dbd980f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Here's a brief report on Retrieval-Augmented Generation (RAG) models:\\n\\n**What is RAG?**\\n\\nRetrieval-Augmented Generation (RAG) is a type of machine learning model that combines the strengths of two approaches: Retrieval-based models and Generative models. RAG models are designed to address the limitations of traditional generative models, which can be prone to hallucinations or generate uninformative text.\\n\\n**How does RAG work?**\\n\\nA RAG model typically consists of two main components:\\n\\n1. **Retrieval component**: This component uses a set of stored documents (e.g., books, articles) and a retrieval algorithm (e.g., BM25, TF-IDF) to identify relevant snippets or passages from the document collection that match the input query or prompt.\\n2. **Generation component**: This component generates text based on the retrieved passages, often using a sequence-to-sequence model (e.g., transformer-based models like BART or T5).\\n\\n**Key benefits of RAG**\\n\\n1. **Improved accuracy and relevance**: By leveraging stored knowledge from the retrieval component, RAG models can generate more accurate and relevant responses.\\n2. **Reduced hallucinations**: Since RAG models rely on external knowledge rather than purely generated text, they are less prone to hallucinations or generating uninformative text.\\n3. **Efficient use of memory**: RAG models only retrieve relevant passages from the document collection, reducing memory requirements compared to traditional generative models.\\n\\n**Examples and applications**\\n\\nRAG models have been applied in various domains, including:\\n\\n1. **Question answering (QA)**: RAG models can be used to answer complex questions by retrieving relevant passages from a large corpus.\\n2. **Text summarization**: RAG models can generate summary text that is informed by the retrieved passages.\\n3. **Dialogue systems**: RAG models can improve dialogue systems by leveraging stored knowledge and context.\\n\\n**Challenges and limitations**\\n\\n1. **Data quality and availability**: The quality and availability of the document collection used for retrieval can significantly impact the performance of RAG models.\\n2. **Computational efficiency**: RAG models require efficient retrieval algorithms to avoid slowing down the generation component.\\n3. **Adversarial attacks**: RAG models may be vulnerable to adversarial attacks that manipulate the input query or prompt.\\n\\nOverall, Retrieval-Augmented Generation (RAG) is a promising approach for generating text that is informed by external knowledge and context, with applications in various domains such as QA, summarization, and dialogue systems.\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = prompt | llm | output_parser\n",
    "chain.invoke(\"Retrieval Augmentatiion Generation\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1ba174b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_fact(x):\n",
    "    if '\\n\\n' in x:\n",
    "        return '\\n'.join(x.split('\\n\\n')[1:])\n",
    "    return x\n",
    "old_word_1= 'RAG'\n",
    "old_word_2= 'Retrieval'\n",
    "new_word= 'Traditional RAG'\n",
    "def replace_word(x):\n",
    "    return x.replace(old_word_1, new_word).replace(old_word_2, new_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6ce64077",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "54c66958",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**What is Traditional RAG Augmented Generation (Traditional RAG)?**\n",
       "Traditional RAG Augmented Generation (Traditional RAG) is a class of neural architectures that combine the strengths of traditional retrievals (e.g., database querying, search engines) with the flexibility and creativity of language models. Traditional RAG systems aim to bridge the gap between explicit data storage and implicit knowledge representation in neural networks.\n",
       "**Key Components**\n",
       "1. **Traditional RAG Module**: This module searches a database or knowledge graph for relevant information related to a given query or input prompt.\n",
       "2. **Generator Module**: A pre-trained language model (e.g., transformer, BART) that generates text based on the retrieved information and the input prompt.\n",
       "**How it Works**\n",
       "1. The retrieval module receives an input prompt and searches for relevant documents, entities, or relationships in a database or knowledge graph.\n",
       "2. The generator module takes the retrieved information and the original input prompt as inputs to generate text that incorporates the retrieved information.\n",
       "**Advantages**\n",
       "1. **Improved Accuracy**: Traditional RAG systems can leverage external knowledge to improve accuracy and reduce reliance on pre-existing text generation capabilities.\n",
       "2. **Increased Contextual Understanding**: By incorporating external knowledge, Traditional RAG models can better understand context and relationships between entities.\n",
       "3. **Flexibility**: Traditional RAG architectures can be applied to various tasks, such as question answering, text summarization, or conversational dialogue.\n",
       "**Applications**\n",
       "1. **Question Answering**: Traditional RAG systems can retrieve relevant documents and generate answers based on the retrieved information.\n",
       "2. **Text Summarization**: Traditional RAG models can summarize long documents by retrieving key points and generating a concise summary.\n",
       "3. **Conversational Dialogue**: Traditional RAG systems can engage in conversations by retrieving relevant context and generating responses.\n",
       "**Challenges**\n",
       "1. **Knowledge Graph Construction**: Building and maintaining high-quality knowledge graphs is crucial for effective retrieval-augmented generation.\n",
       "2. **Scalability**: Traditional RAG systems need to handle large-scale data and maintain efficiency as the database or knowledge graph grows.\n",
       "3. **Evaluation Metrics**: Developing suitable evaluation metrics that capture the strengths of Traditional RAG systems remains an open research challenge.\n",
       "**State-of-the-Art**\n",
       "Recent works have shown promising results on various tasks, including question answering (e.g., BART + RETRO) and text summarization (e.g., T5 + Retrieve-Generator). However, more research is needed to fully unlock the potential of retrieval augmented generation."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chain  = prompt | llm | output_parser | RunnableLambda(extract_fact) | RunnableLambda(replace_word)\n",
    "result = chain.invoke(\"retrieval augmented generation\")\n",
    "display(Markdown(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "510f6e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import DocArrayInMemorySearch\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8c4d40c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting docarray\n",
      "  Downloading docarray-0.41.0-py3-none-any.whl.metadata (36 kB)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /Volumes/Development/Practise/se-practises/.venv/lib/python3.13/site-packages (from docarray) (2.3.5)\n",
      "Requirement already satisfied: orjson>=3.8.2 in /Volumes/Development/Practise/se-practises/.venv/lib/python3.13/site-packages (from docarray) (3.11.4)\n",
      "Requirement already satisfied: pydantic>=1.10.8 in /Volumes/Development/Practise/se-practises/.venv/lib/python3.13/site-packages (from docarray) (2.12.4)\n",
      "Collecting rich>=13.1.0 (from docarray)\n",
      "  Downloading rich-14.2.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting types-requests>=2.28.11.6 (from docarray)\n",
      "  Downloading types_requests-2.32.4.20250913-py3-none-any.whl.metadata (2.0 kB)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in /Volumes/Development/Practise/se-practises/.venv/lib/python3.13/site-packages (from docarray) (0.9.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Volumes/Development/Practise/se-practises/.venv/lib/python3.13/site-packages (from pydantic>=1.10.8->docarray) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /Volumes/Development/Practise/se-practises/.venv/lib/python3.13/site-packages (from pydantic>=1.10.8->docarray) (2.41.5)\n",
      "Requirement already satisfied: typing-extensions>=4.14.1 in /Volumes/Development/Practise/se-practises/.venv/lib/python3.13/site-packages (from pydantic>=1.10.8->docarray) (4.15.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /Volumes/Development/Practise/se-practises/.venv/lib/python3.13/site-packages (from pydantic>=1.10.8->docarray) (0.4.2)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich>=13.1.0->docarray)\n",
      "  Downloading markdown_it_py-4.0.0-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Volumes/Development/Practise/se-practises/.venv/lib/python3.13/site-packages (from rich>=13.1.0->docarray) (2.19.2)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=13.1.0->docarray)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: urllib3>=2 in /Volumes/Development/Practise/se-practises/.venv/lib/python3.13/site-packages (from types-requests>=2.28.11.6->docarray) (2.5.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /Volumes/Development/Practise/se-practises/.venv/lib/python3.13/site-packages (from typing-inspect>=0.8.0->docarray) (1.1.0)\n",
      "Downloading docarray-0.41.0-py3-none-any.whl (302 kB)\n",
      "Downloading rich-14.2.0-py3-none-any.whl (243 kB)\n",
      "Downloading markdown_it_py-4.0.0-py3-none-any.whl (87 kB)\n",
      "Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Downloading types_requests-2.32.4.20250913-py3-none-any.whl (20 kB)\n",
      "Installing collected packages: types-requests, mdurl, markdown-it-py, rich, docarray\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5/5\u001b[0m [docarray]4/5\u001b[0m [docarray]\n",
      "\u001b[1A\u001b[2KSuccessfully installed docarray-0.41.0 markdown-it-py-4.0.0 mdurl-0.1.2 rich-14.2.0 types-requests-2.32.4.20250913\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install docarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fc312439",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, HumanMessagePromptTemplate, SystemMessagePromptTemplate\n",
    "prompt_str = \"\"\"Using the context provided, answer user's question\n",
    "context:\n",
    "{context1}\n",
    "{context2}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_messages([SystemMessagePromptTemplate.from_template(prompt_str), HumanMessagePromptTemplate.from_template(\"{question}\")])\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "vector_store_a = DocArrayInMemorySearch.from_texts(texts=[\"half the info is here\", \"Deepseek-V3 was released in December 2024\"], embedding=embeddings)\n",
    "vector_store_b = DocArrayInMemorySearch.from_texts(texts=[\"the otehr half of the info is here\", \"the deepseek-v3 llm is a mixture of experts with 671B parameters\"], embedding=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a3e399c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "62131958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps__={'context1': VectorStoreRetriever(tags=['DocArrayInMemorySearch'], vectorstore=<langchain_community.vectorstores.docarray.in_memory.DocArrayInMemorySearch object at 0x14930dbd0>, search_kwargs={}), 'context2': VectorStoreRetriever(tags=['DocArrayInMemorySearch'], vectorstore=<langchain_community.vectorstores.docarray.in_memory.DocArrayInMemorySearch object at 0x14930dd10>, search_kwargs={}), 'question': RunnablePassthrough()}\n"
     ]
    }
   ],
   "source": [
    "retrieval = RunnableParallel(\n",
    "    {\n",
    "        \"context1\": vector_store_a.as_retriever(),\n",
    "        \"context2\": vector_store_b.as_retriever(),\n",
    "        \"question\": RunnablePassthrough()\n",
    "    }\n",
    ")\n",
    "print(retrieval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45697814",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = retrieval | prompt | llm | output_parser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "24080646",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The DeepSeek-V3 model uses a \"Mixture of Experts\" (MoE) architecture, and it has 671B parameters.', additional_kwargs={}, response_metadata={'model': 'llama3.1:8b', 'created_at': '2025-11-22T09:56:12.800188Z', 'message': {'role': 'assistant', 'content': ''}, 'done': True, 'done_reason': 'stop', 'total_duration': 1462491916, 'load_duration': 116734000, 'prompt_eval_count': 111, 'prompt_eval_duration': 198258959, 'eval_count': 29, 'eval_duration': 1106103667}, id='lc_run--1d8eb1b1-6773-4cac-9316-800bb5e19669-0')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"What architecture does deepseek model released in december uses?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cbef694",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
